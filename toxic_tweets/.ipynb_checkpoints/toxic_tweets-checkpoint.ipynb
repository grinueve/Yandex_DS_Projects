{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "477424f5",
   "metadata": {},
   "source": [
    "# Проект по теме \"Машинное обучение для текстов\"\n",
    "## 1. Описание проекта\n",
    "\n",
    "**Проект для «Викишоп»**\n",
    "\n",
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, другими словами, клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. Необходимо построить модель детектирования негативных сообщений для дальнейшей обработки сервисной службой.\n",
    "\n",
    "**Цели**:\n",
    "Обучить модель классифицировать комментарии на позитивные и негативные. В распоряжении набор данных с разметкой о токсичности сообщений. Построить модель со значением метрики качества F1 не меньше 0.75.\n",
    "\n",
    "**Инструкция по выполнению проекта**:\n",
    "- Загрузить и подготовить данные.\n",
    "- Обучить разные модели.\n",
    "- Сделать выводы.\n",
    "\n",
    "### 1.1. Описание данных\n",
    "Данные находятся в файле **toxic_comments.csv.**\n",
    "\n",
    "**text** - текст комментария,\n",
    "**toxic** - целевой признак.\n",
    "\n",
    "**План выполнения работы:**\n",
    "\n",
    "- Подготовка Данных\n",
    "- Обучение моделей\n",
    "    - 2.1 Logistic Regression\n",
    "    - 2.2 NB-SVM\n",
    "    - 2.3 Linear SVC\n",
    "- Выводы\n",
    "\n",
    "### 1.2. Подготовка данных\n",
    "Подключаем библиотеки:\n",
    "- **pandas** - для работы с таблицами\n",
    "- **sklearn** - инструменты машинного обучения (модели классификации, метрики для исследования качества моделей, разделение данных, предобработка данных)\n",
    "- **nltk** - для лемматизации и фильтрации стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae698dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer as WNL\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "import torch\n",
    "import transformers\n",
    "from tqdm import notebook\n",
    "from tqdm.notebook import tqdm\n",
    "from wordcloud import WordCloud\n",
    "from time import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcef711e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>127667</th>\n",
       "      <td>127799</td>\n",
       "      <td>stfu ==\\n\\nWHY DONT U STFU AND GET A LIFE! IF ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84420</th>\n",
       "      <td>84501</td>\n",
       "      <td>I don't know whether the map per se is correct...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129212</th>\n",
       "      <td>129345</td>\n",
       "      <td>Two as in disambiguation?  Farmbrough, .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142503</th>\n",
       "      <td>142656</td>\n",
       "      <td>UNBLOCK - the question of integrity is not an ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102047</th>\n",
       "      <td>102144</td>\n",
       "      <td>Please remember to sign your posts by adding f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                               text  toxic\n",
       "127667      127799  stfu ==\\n\\nWHY DONT U STFU AND GET A LIFE! IF ...      1\n",
       "84420        84501  I don't know whether the map per se is correct...      0\n",
       "129212      129345           Two as in disambiguation?  Farmbrough, .      0\n",
       "142503      142656  UNBLOCK - the question of integrity is not an ...      0\n",
       "102047      102144  Please remember to sign your posts by adding f...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    toxic_data = pd.read_csv('toxic.csv')\n",
    "except:\n",
    "    toxic_data = pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv')\n",
    "    \n",
    "toxic_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3385584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "toxic_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6601ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_data = toxic_data.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "631dbd48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159292, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45569609",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_data['text'] = toxic_data['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d766793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de71fd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_data = toxic_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ebb8608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "toxic    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91300785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процент объектов класса 1 к общему объёму датасета: 10.15%\n"
     ]
    }
   ],
   "source": [
    "toxic_data['toxic'].value_counts()\n",
    "print(f\"Процент объектов класса 1 к общему объёму датасета: {(sum(toxic_data['toxic']) / len(toxic_data) * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da453d02",
   "metadata": {},
   "source": [
    "###   1.3. Лемматизация с помощью WordNetLemmatizer\n",
    "\n",
    "- Удалим пунктуацию и лишние пробелы\n",
    "- Удалим стоп-слова \n",
    "- Проведём лемматизацию слов с помощью WordNetLemmatizer() из библиотеки nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2af703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):    \n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14af3e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WNL()\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b74e3f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemm_wnl(text):\n",
    "    text = re.sub(r'[^a-zA-z ]', ' ', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    text = ' '.join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b279d303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The strip bat be hang on their foot for best'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemm_wnl(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe82fd9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the stripe bat be hang on their foot for good'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "def lemm_spacy(text):\n",
    "    text = re.sub(r'[^a-zA-z ]', ' ', text)\n",
    "    doc = nlp(text)\n",
    "    text = ' '.join([token.lemma_ for token in doc])\n",
    "    return text\n",
    "\n",
    "lemm_spacy(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20c208e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36a821e1d8843abb05c02eb650b26e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "toxic_data['lemm_text_WML'] = toxic_data['text'].progress_apply(lemm_wnl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "618b47f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemm_text_WML</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44289</th>\n",
       "      <td>good intentions \\n\\nhi yan,\\n\\ni fail to see t...</td>\n",
       "      <td>0</td>\n",
       "      <td>good intention hi yan i fail to see the good i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44949</th>\n",
       "      <td>\"\\n\\n why are you removing the only remaining ...</td>\n",
       "      <td>0</td>\n",
       "      <td>why be you remove the only remain prose look i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102408</th>\n",
       "      <td>\"auto|1=65.242.111.254|2=autoblocked because y...</td>\n",
       "      <td>0</td>\n",
       "      <td>auto autoblocked because your ip address be re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36375</th>\n",
       "      <td>it makes me feel better, and i like that. i ne...</td>\n",
       "      <td>0</td>\n",
       "      <td>it make me feel well and i like that i never t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97115</th>\n",
       "      <td>you don't and yet while you ask that others o ...</td>\n",
       "      <td>0</td>\n",
       "      <td>you don t and yet while you ask that others o ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic  \\\n",
       "44289   good intentions \\n\\nhi yan,\\n\\ni fail to see t...      0   \n",
       "44949   \"\\n\\n why are you removing the only remaining ...      0   \n",
       "102408  \"auto|1=65.242.111.254|2=autoblocked because y...      0   \n",
       "36375   it makes me feel better, and i like that. i ne...      0   \n",
       "97115   you don't and yet while you ask that others o ...      0   \n",
       "\n",
       "                                            lemm_text_WML  \n",
       "44289   good intention hi yan i fail to see the good i...  \n",
       "44949   why be you remove the only remain prose look i...  \n",
       "102408  auto autoblocked because your ip address be re...  \n",
       "36375   it make me feel well and i like that i never t...  \n",
       "97115   you don t and yet while you ask that others o ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7cf3019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be6999511a14017869629d24359aabf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toxic_data['lemm_text_spacy'] = toxic_data['text'].progress_apply(lemm_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bc52103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemm_text_WML</th>\n",
       "      <th>lemm_text_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>151726</th>\n",
       "      <td>this article needs a rewrite. \\n\\ni did some c...</td>\n",
       "      <td>0</td>\n",
       "      <td>this article need a rewrite i do some cleaning...</td>\n",
       "      <td>this article need a rewrite     I do some clea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>y r we doing this: pat</td>\n",
       "      <td>0</td>\n",
       "      <td>y r we do this pat</td>\n",
       "      <td>y r we do this   pat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74011</th>\n",
       "      <td>south house \\n\\ni was sure he was in south hou...</td>\n",
       "      <td>0</td>\n",
       "      <td>south house i be sure he be in south house in ...</td>\n",
       "      <td>south house    I be sure he be in south house ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72852</th>\n",
       "      <td>defamatory articles \\n\\nyou are behind the def...</td>\n",
       "      <td>0</td>\n",
       "      <td>defamatory article you be behind the defamator...</td>\n",
       "      <td>defamatory article    you be behind the defama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36701</th>\n",
       "      <td>do i know you? ==because you are a fggt!\\n do ...</td>\n",
       "      <td>1</td>\n",
       "      <td>do i know you because you be a fggt do i know ...</td>\n",
       "      <td>do I know you     because you be a fggt    do ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic  \\\n",
       "151726  this article needs a rewrite. \\n\\ni did some c...      0   \n",
       "259                                y r we doing this: pat      0   \n",
       "74011   south house \\n\\ni was sure he was in south hou...      0   \n",
       "72852   defamatory articles \\n\\nyou are behind the def...      0   \n",
       "36701   do i know you? ==because you are a fggt!\\n do ...      1   \n",
       "\n",
       "                                            lemm_text_WML  \\\n",
       "151726  this article need a rewrite i do some cleaning...   \n",
       "259                                    y r we do this pat   \n",
       "74011   south house i be sure he be in south house in ...   \n",
       "72852   defamatory article you be behind the defamator...   \n",
       "36701   do i know you because you be a fggt do i know ...   \n",
       "\n",
       "                                          lemm_text_spacy  \n",
       "151726  this article need a rewrite     I do some clea...  \n",
       "259                                  y r we do this   pat  \n",
       "74011   south house    I be sure he be in south house ...  \n",
       "72852   defamatory article    you be behind the defama...  \n",
       "36701   do I know you     because you be a fggt    do ...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cb47314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#toxic_data = toxic_data.drop('Unnamed: 0', axis=1)\n",
    "toxic_data.rename(columns={\"lemm_text_WML\": \"lemm_text_WNL\"}, inplace=True)\n",
    "\n",
    "features_wnl = toxic_data['lemm_text_WNL']\n",
    "features_spacy = toxic_data['lemm_text_spacy']\n",
    "target = toxic_data['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e4b6f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Разделим данные на тренировочную и тестовую выборки\n",
    "RANDOM_STATE, CV = 1234, 5\n",
    "features_train_wnl, features_test_wnl,\\\n",
    "features_train_spacy, features_test_spacy,\\\n",
    "target_train, target_test = train_test_split(features_wnl, features_spacy, target,\n",
    "                                             test_size=0.1, stratify=target, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38331589",
   "metadata": {},
   "source": [
    "## 2. Обучение через Pipelines\n",
    "\n",
    "Найдём метрику f1 для константной модели **Dummy**. Будем предсказывать все твиты нетоксичными ('toxic'=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e4d1de",
   "metadata": {},
   "source": [
    "### 2.1. DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "465ed34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['constant', 'random_state', 'strategy'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dm = DummyClassifier()\n",
    "model_dm.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85b4739e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(stop_words=['don', \"it's\", 'have', 'on',\n",
       "                                             'each', 'some', 'yourself', 'any',\n",
       "                                             'the', \"mustn't\", 'down', 'hers',\n",
       "                                             'shan', 't', 'wouldn', 'is',\n",
       "                                             \"mightn't\", \"hasn't\", 'does',\n",
       "                                             'wasn', 'at', 'why', 'where',\n",
       "                                             \"aren't\", 'because', 'be', 'both',\n",
       "                                             'can', \"don't\", 'yours', ...])),\n",
       "                ('model_dm', DummyClassifier(random_state=1234))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "pipeline_dc = Pipeline(\n",
    "    [\n",
    "        (\"vectorizer\", TfidfVectorizer(stop_words=list(stop_words))),\n",
    "        (\"model_dm\", DummyClassifier(random_state=RANDOM_STATE)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameter_grid_dc = dict(\n",
    "    #vectorizer__ngram_range = [(1, 1), (1, 2)],\n",
    "    model_dm__strategy = ['most_frequent', 'uniform'],\n",
    ")\n",
    "\n",
    "pipeline_dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21f7de91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RSCV(pipeline, grid, features_train):\n",
    "    \n",
    "    rscv = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=grid,\n",
    "    scoring='f1',\n",
    "    cv=CV,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    error_score='raise')\n",
    "    \n",
    "    t0 = time()\n",
    "    rscv.fit(features_train, target_train)\n",
    "    print(f\"Done in {time() - t0:.3f}s\")\n",
    "    print(f\"Best parameters combination found:\")\n",
    "    best_parameters = rscv.best_estimator_.get_params()\n",
    "    \n",
    "    for param_name in sorted(grid.keys()):\n",
    "        print(f\"{param_name}: {best_parameters[param_name]}\")\n",
    "    print(f\"Best f1: {rscv.best_score_:.3f}\")\n",
    "    \n",
    "    return rscv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ae97860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Done in 70.884s\n",
      "Best parameters combination found:\n",
      "model_dm__strategy: uniform\n",
      "Best f1: 0.168\n"
     ]
    }
   ],
   "source": [
    "dc_rscv_wnl = RSCV(pipeline_dc, parameter_grid_dc, features_train_wnl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4eda382e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Done in 75.545s\n",
      "Best parameters combination found:\n",
      "model_dm__strategy: uniform\n",
      "Best f1: 0.168\n"
     ]
    }
   ],
   "source": [
    "dc_rscv_spacy = RSCV(pipeline_dc, parameter_grid_dc, features_train_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2534657a",
   "metadata": {},
   "source": [
    "### 2.2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d12ace4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['C', 'class_weight', 'dual', 'fit_intercept', 'intercept_scaling', 'l1_ratio', 'max_iter', 'multi_class', 'n_jobs', 'penalty', 'random_state', 'solver', 'tol', 'verbose', 'warm_start'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logr = LogisticRegression()\n",
    "model_logr.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f0fa92",
   "metadata": {},
   "source": [
    "Обучение, подбор гиперпараметров логистической регрессии и кросс-валидацию проведём с помощью GridSearchCV, подбирать будем гиперпараметры **С и max_iter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78fa332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_logr = Pipeline(\n",
    "    [\n",
    "        (\"vectorizer\", TfidfVectorizer(stop_words=list(stop_words))),\n",
    "        (\"model_logr\", LogisticRegression(random_state=RANDOM_STATE, class_weight='balanced')),\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameter_grid_logr = dict(\n",
    "    #vectorizer__ngram_range = [(1, 1), (1, 2)],\n",
    "    model_logr__C = [10, 20],\n",
    "    model_logr__solver = ['liblinear', 'lbfgs']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf44d54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Done in 950.148s\n",
      "Best parameters combination found:\n",
      "model_logr__C: 10\n",
      "model_logr__solver: liblinear\n",
      "Best f1: 0.756\n"
     ]
    }
   ],
   "source": [
    "logr_rscv_wnl = RSCV(pipeline_logr, parameter_grid_logr, features_train_wnl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7eb91768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Done in 1060.104s\n",
      "Best parameters combination found:\n",
      "model_logr__C: 10\n",
      "model_logr__solver: lbfgs\n",
      "Best f1: 0.763\n"
     ]
    }
   ],
   "source": [
    "logr_rscv_spacy = RSCV(pipeline_logr, parameter_grid_logr, features_train_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8467ba22",
   "metadata": {},
   "source": [
    "### 2.3. SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7eaf1106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['alpha', 'average', 'class_weight', 'early_stopping', 'epsilon', 'eta0', 'fit_intercept', 'l1_ratio', 'learning_rate', 'loss', 'max_iter', 'n_iter_no_change', 'n_jobs', 'penalty', 'power_t', 'random_state', 'shuffle', 'tol', 'validation_fraction', 'verbose', 'warm_start'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sgdc = SGDClassifier()\n",
    "model_sgdc.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47628c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_sgdc = Pipeline(\n",
    "    [\n",
    "        (\"vectorizer\", TfidfVectorizer(stop_words=list(stop_words))),\n",
    "        (\"model_sgdc\", SGDClassifier(random_state=RANDOM_STATE, class_weight='balanced', \n",
    "                                     learning_rate='optimal', penalty='l2')),\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameter_grid_sgdc = dict(\n",
    "    #vectorizer__ngram_range = [(1, 1), (1, 2)],\n",
    "    model_sgdc__loss = ['modified_huber', 'perceptron'],\n",
    "    model_sgdc__alpha = [0.1, 0.2],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f98e722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Done in 521.313s\n",
      "Best parameters combination found:\n",
      "model_sgdc__alpha: 0.2\n",
      "model_sgdc__loss: perceptron\n",
      "Best f1: 0.690\n"
     ]
    }
   ],
   "source": [
    "sgdc_rscv_wnl = RSCV(pipeline_sgdc, parameter_grid_sgdc, features_train_wnl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f020c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Done in 984.403s\n",
      "Best parameters combination found:\n",
      "model_sgdc__alpha: 0.2\n",
      "model_sgdc__loss: perceptron\n",
      "Best f1: 0.701\n"
     ]
    }
   ],
   "source": [
    "sgdc_rscv_spacy = RSCV(pipeline_sgdc, parameter_grid_sgdc, features_train_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64474e21",
   "metadata": {},
   "source": [
    "Данная модель быстрее Логистической регрессии в три раза, но значение f1 меньше на 0.015, но всё ещё выше ТЗ.\n",
    "\n",
    "### 2.4. MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5de0d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['alpha', 'class_prior', 'fit_prior'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model_mnb = MultinomialNB()\n",
    "model_mnb.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea463a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_mnb = Pipeline(\n",
    "    [\n",
    "        (\"vectorizer\", TfidfVectorizer(stop_words=list(stop_words))),\n",
    "        (\"model_mnb\", MultinomialNB()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameter_grid_mnb = dict(\n",
    "    #vectorizer__ngram_range = [(1, 1), (1, 2)],\n",
    "    model_mnb__alpha = [0.1, 0.2, 0.5],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31c2906a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Done in 748.005s\n",
      "Best parameters combination found:\n",
      "model_mnb__alpha: 0.1\n",
      "Best f1: 0.629\n"
     ]
    }
   ],
   "source": [
    "mnb_rscv_wnl = RSCV(pipeline_mnb, parameter_grid_mnb, features_train_wnl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "367d0f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Done in 815.903s\n",
      "Best parameters combination found:\n",
      "model_mnb__alpha: 0.1\n",
      "Best f1: 0.641\n"
     ]
    }
   ],
   "source": [
    "mnb_rscv_spacy = RSCV(pipeline_mnb, parameter_grid_mnb, features_train_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94188d94",
   "metadata": {},
   "source": [
    "### 2.5. Лучшая модель на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2ee59a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Показатель f1 на тестовой выборке: 0.780\n",
      "Accuracy на логистической регрессии 0.952\n"
     ]
    }
   ],
   "source": [
    "logr_predict = logr_rscv_spacy.predict(features_test_spacy)\n",
    "f1_lr = f1_score(target_test, logr_predict)\n",
    "print(f\"Показатель f1 на тестовой выборке: {f1_lr:.3f}\")\n",
    "accuracy_lr = accuracy_score(target_test, logr_predict)\n",
    "print(f\"Accuracy на логистической регрессии {accuracy_lr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75be496",
   "metadata": {},
   "source": [
    "- Лемматизация spacy точнее и быстрее.\n",
    "- Показатель f1=0.78 на тестовой выборке удовлетворяет условию задачи.\n",
    "- Accuracy на тестовой выборке у логистической регрессии составляет 0.952"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a88f2ab",
   "metadata": {},
   "source": [
    "## 3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631d7f4c",
   "metadata": {},
   "source": [
    "- Данные о токсичности твитов успешно загружены и обработаны:\n",
    "- Лемматизация проведена с помощью **WordNetLemmatizer** библиотеки nltk и **Spacy**\n",
    "- Знаки пунктуации, а также лишние пробелы удалены\n",
    "- Стоп слова удалены в процессе векторизации(список взят из библиотеки nltk)\n",
    "- Корпус обработран с помощью pipelin-ов, содержащих **TfidfVectorizer** и одну из трёх наиболее распространённых моделей классификации текста: **LogisticRegression, SCDG, MultinomialNB**\n",
    "- f1 моделей в диапазоне 0.63-0.76. Максимальный показатель f1 на тестовой выборке получен для **LogisticRegression**: 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34a00fd",
   "metadata": {},
   "source": [
    "## 4. BERT на выборке (для тренировки)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d55218",
   "metadata": {},
   "source": [
    "### 4.1. Подготовка данных и увеличение скорости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "998af175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 17:07:17.778161: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()\n",
    "tf.test.is_built_with_cuda()\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2a579942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "497d20b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процент объектов класса 1 к общему объёму датасета: 9.84%\n"
     ]
    }
   ],
   "source": [
    "toxic_data_5000 = toxic_data.sample(5000).reset_index(drop=True)\n",
    "print(f\"Процент объектов класса 1 к общему объёму датасета: {(sum(toxic_data_5000['toxic'])/50):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3a8441",
   "metadata": {},
   "source": [
    "### 4.2. BERT-токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "db3c4a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dabfd8b2aeb438db22025b92e575fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16d3d1e655748b5acba863dd999c9dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17b4f485cf3415582e0bfb466e5937b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783c2deb83d54f3789fc85b3fefa2a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "tokenized = toxic_data_5000['text'].apply(\n",
    "    lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=64, truncation=True))\n",
    "\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a250f46",
   "metadata": {},
   "source": [
    "### 4.3. Загрузка DistillBERT модели и создание эмбедингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8479c70a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24e58482ab2482dab39506f8fe14467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff53739f05b4281a42452eeb0256304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/251M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_bert = transformers.DistilBertModel.from_pretrained('distilbert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac3b37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed65aa17c624870aa9e1764afb1fa40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 5\n",
    "embeddings = []\n",
    "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "    batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]) \n",
    "    attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        batch_embeddings = model_bert(batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "    embeddings.append(batch_embeddings[0][:,0,:].cpu().numpy())\n",
    "    del batch, attention_mask_batch, batch_embeddings\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95793e5",
   "metadata": {},
   "source": [
    "### 4.4 Испытание лучшей модели на эмбедингах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83967701",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = np.concatenate(embeddings)\n",
    "target_train = toxic_data_5000['toxic']\n",
    "features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5d486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_logr = Pipeline(\n",
    "    [\n",
    "        #(\"vectorizer\", TfidfVectorizer(stop_words=list(stop_words))),\n",
    "        (\"model_logr\", LogisticRegression(random_state=RANDOM_STATE, class_weight='balanced')),\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameter_grid_logr = dict(\n",
    "    #vectorizer__ngram_range = [(1, 1), (1, 2)],\n",
    "    model_logr__C = [10, 20],\n",
    "    model_logr__solver = ['liblinear', 'lbfgs']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426743d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logr_rscv = RSCV(pipeline_logr, parameter_grid_logr, features_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e3a483",
   "metadata": {},
   "source": [
    "f1 = 0.652"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618a9162",
   "metadata": {},
   "source": [
    "## 5. Итоговый вывод\n",
    "**В ходе работы над проектом было сделано:**\n",
    "- Подготовлены данные для обучения на моделях.\n",
    "- Данные разделены на обучающую и тестовою выборку в соотношении 9:1.\n",
    "- Обучены модели через пайплайн с векторизацией и выбраны лучшие из них валидацией.\n",
    "- Исследованы параметры качества моделей.\n",
    "\n",
    "- Исходные данные обладают большим количеством признаков. Так как TF-IDF превращают текст в численные значения, лучшими моделями стали LogisticRegression и SGDClassifier.\n",
    "\n",
    "- На тестовой выбоке по метрике F1 лучше всего себя показал LogisticRegression.\n",
    "\n",
    "- Для небольшой (в 5000 ед.) выборки была проведена токенизацация и векторизация на DistillBERT-ом имеющимися мощностями. Преобразованные данные были классифицированы LogisticRegression. Значения f1 и accuracy получились незначительными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bba259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 16610,
    "start_time": "2023-08-03T21:24:35.813Z"
   },
   {
    "duration": 268,
    "start_time": "2023-08-03T21:24:52.426Z"
   },
   {
    "duration": 20789,
    "start_time": "2023-08-07T15:36:41.723Z"
   },
   {
    "duration": 0,
    "start_time": "2023-08-07T15:37:02.516Z"
   },
   {
    "duration": 868,
    "start_time": "2023-08-07T15:39:09.396Z"
   },
   {
    "duration": 303,
    "start_time": "2023-08-07T15:39:45.853Z"
   },
   {
    "duration": 4996,
    "start_time": "2023-08-07T15:39:50.757Z"
   },
   {
    "duration": 2316,
    "start_time": "2023-08-07T15:40:37.750Z"
   },
   {
    "duration": 41,
    "start_time": "2023-08-07T15:40:42.203Z"
   },
   {
    "duration": 16,
    "start_time": "2023-08-07T15:40:43.010Z"
   },
   {
    "duration": 4,
    "start_time": "2023-08-07T15:40:43.535Z"
   },
   {
    "duration": 386,
    "start_time": "2023-08-07T15:40:43.993Z"
   },
   {
    "duration": 275,
    "start_time": "2023-08-07T15:40:47.317Z"
   },
   {
    "duration": 259,
    "start_time": "2023-08-07T15:40:48.248Z"
   },
   {
    "duration": 27,
    "start_time": "2023-08-07T15:40:48.512Z"
   },
   {
    "duration": 28,
    "start_time": "2023-08-07T15:40:48.997Z"
   },
   {
    "duration": 212,
    "start_time": "2023-08-07T15:40:49.416Z"
   },
   {
    "duration": 37558,
    "start_time": "2023-08-07T15:40:49.631Z"
   },
   {
    "duration": 14794,
    "start_time": "2023-08-07T15:44:03.173Z"
   },
   {
    "duration": 44761,
    "start_time": "2023-08-07T15:44:30.989Z"
   },
   {
    "duration": 30042,
    "start_time": "2023-08-07T15:45:47.905Z"
   },
   {
    "duration": 2373,
    "start_time": "2023-08-07T15:46:28.324Z"
   },
   {
    "duration": 49,
    "start_time": "2023-08-07T15:46:30.700Z"
   },
   {
    "duration": 29,
    "start_time": "2023-08-07T15:46:31.378Z"
   },
   {
    "duration": 7,
    "start_time": "2023-08-07T15:46:31.882Z"
   },
   {
    "duration": 438,
    "start_time": "2023-08-07T15:46:32.050Z"
   },
   {
    "duration": 272,
    "start_time": "2023-08-07T15:46:33.201Z"
   },
   {
    "duration": 279,
    "start_time": "2023-08-07T15:46:33.475Z"
   },
   {
    "duration": 32,
    "start_time": "2023-08-07T15:46:33.917Z"
   },
   {
    "duration": 30,
    "start_time": "2023-08-07T15:46:34.485Z"
   },
   {
    "duration": 260,
    "start_time": "2023-08-07T15:46:40.940Z"
   },
   {
    "duration": 39509,
    "start_time": "2023-08-07T15:46:46.046Z"
   },
   {
    "duration": 4,
    "start_time": "2023-08-07T15:48:23.525Z"
   },
   {
    "duration": 598,
    "start_time": "2023-08-07T15:48:24.118Z"
   },
   {
    "duration": 4,
    "start_time": "2023-08-07T15:48:40.833Z"
   },
   {
    "duration": 125,
    "start_time": "2023-08-07T15:48:42.305Z"
   },
   {
    "duration": 841,
    "start_time": "2023-08-07T15:49:38.702Z"
   },
   {
    "duration": 1979,
    "start_time": "2023-08-07T15:49:50.686Z"
   },
   {
    "duration": 1072,
    "start_time": "2023-08-07T15:50:01.842Z"
   },
   {
    "duration": 1508262,
    "start_time": "2023-08-07T15:52:01.150Z"
   },
   {
    "duration": 13,
    "start_time": "2023-08-07T17:08:25.670Z"
   },
   {
    "duration": 974,
    "start_time": "2023-08-07T17:08:30.513Z"
   },
   {
    "duration": 4,
    "start_time": "2023-08-07T17:08:57.403Z"
   },
   {
    "duration": 1325381,
    "start_time": "2023-08-07T17:09:03.887Z"
   },
   {
    "duration": 20,
    "start_time": "2023-08-07T17:31:09.270Z"
   },
   {
    "duration": 2774,
    "start_time": "2023-08-07T19:27:25.437Z"
   },
   {
    "duration": 5146,
    "start_time": "2023-08-07T19:27:33.297Z"
   },
   {
    "duration": 2402,
    "start_time": "2023-08-07T19:27:55.916Z"
   },
   {
    "duration": 57,
    "start_time": "2023-08-07T19:31:23.274Z"
   },
   {
    "duration": 1867,
    "start_time": "2023-08-07T19:32:28.555Z"
   },
   {
    "duration": 38,
    "start_time": "2023-08-07T19:32:35.115Z"
   },
   {
    "duration": 17,
    "start_time": "2023-08-07T19:32:39.323Z"
   },
   {
    "duration": 4,
    "start_time": "2023-08-07T19:32:42.303Z"
   },
   {
    "duration": 262,
    "start_time": "2023-08-07T19:32:44.367Z"
   },
   {
    "duration": 232,
    "start_time": "2023-08-07T19:32:46.957Z"
   },
   {
    "duration": 204,
    "start_time": "2023-08-07T19:32:48.365Z"
   },
   {
    "duration": 32,
    "start_time": "2023-08-07T19:32:50.526Z"
   },
   {
    "duration": 13,
    "start_time": "2023-08-07T19:32:52.708Z"
   },
   {
    "duration": 181,
    "start_time": "2023-08-07T19:32:55.506Z"
   },
   {
    "duration": 26445,
    "start_time": "2023-08-07T19:33:01.199Z"
   },
   {
    "duration": 27141,
    "start_time": "2023-08-07T19:35:20.064Z"
   },
   {
    "duration": 77,
    "start_time": "2023-08-07T19:36:37.474Z"
   },
   {
    "duration": 562,
    "start_time": "2023-08-07T19:36:42.355Z"
   },
   {
    "duration": 27633,
    "start_time": "2023-08-07T19:37:04.510Z"
   },
   {
    "duration": 28222,
    "start_time": "2023-08-07T19:37:46.556Z"
   },
   {
    "duration": 29369,
    "start_time": "2023-08-07T19:39:47.064Z"
   },
   {
    "duration": 29588,
    "start_time": "2023-08-07T19:40:23.404Z"
   },
   {
    "duration": 30181,
    "start_time": "2023-08-07T19:42:06.648Z"
   },
   {
    "duration": 3708,
    "start_time": "2023-08-07T19:43:21.504Z"
   },
   {
    "duration": 29951,
    "start_time": "2023-08-07T19:43:30.770Z"
   },
   {
    "duration": 4,
    "start_time": "2023-08-07T19:45:31.409Z"
   },
   {
    "duration": 52,
    "start_time": "2023-08-07T19:45:34.901Z"
   },
   {
    "duration": 2456,
    "start_time": "2023-08-07T19:45:58.713Z"
   },
   {
    "duration": 3,
    "start_time": "2023-08-07T19:46:09.041Z"
   },
   {
    "duration": 6,
    "start_time": "2023-08-07T19:46:14.033Z"
   },
   {
    "duration": 512,
    "start_time": "2023-08-07T19:46:32.141Z"
   },
   {
    "duration": 412,
    "start_time": "2023-08-07T19:47:06.885Z"
   },
   {
    "duration": 1167628,
    "start_time": "2023-08-07T19:51:10.001Z"
   },
   {
    "duration": 39,
    "start_time": "2023-08-07T20:10:41.508Z"
   },
   {
    "duration": 1136478,
    "start_time": "2023-08-07T20:10:54.623Z"
   },
   {
    "duration": 55,
    "start_time": "2023-08-07T20:29:56.017Z"
   },
   {
    "duration": 4,
    "start_time": "2023-08-07T20:30:07.730Z"
   },
   {
    "duration": 88,
    "start_time": "2023-08-07T20:30:12.675Z"
   },
   {
    "duration": 4,
    "start_time": "2023-08-07T20:30:18.527Z"
   },
   {
    "duration": 16,
    "start_time": "2023-08-07T20:30:22.557Z"
   },
   {
    "duration": 3,
    "start_time": "2023-08-07T20:30:30.927Z"
   },
   {
    "duration": 4,
    "start_time": "2023-08-07T20:30:35.426Z"
   },
   {
    "duration": 61078,
    "start_time": "2023-08-07T20:30:38.407Z"
   },
   {
    "duration": 9,
    "start_time": "2023-08-07T20:31:44.950Z"
   },
   {
    "duration": 3,
    "start_time": "2023-08-07T20:31:48.906Z"
   },
   {
    "duration": 4,
    "start_time": "2023-08-07T20:31:55.534Z"
   },
   {
    "duration": 17934,
    "start_time": "2023-08-07T20:31:58.409Z"
   },
   {
    "duration": 5,
    "start_time": "2023-08-07T20:32:37.746Z"
   },
   {
    "duration": 67095,
    "start_time": "2023-08-07T20:32:40.400Z"
   },
   {
    "duration": 69117,
    "start_time": "2023-08-07T20:33:57.691Z"
   },
   {
    "duration": 5,
    "start_time": "2023-08-07T20:35:44.994Z"
   },
   {
    "duration": 5,
    "start_time": "2023-08-07T20:35:51.310Z"
   },
   {
    "duration": 932786,
    "start_time": "2023-08-07T20:37:14.050Z"
   },
   {
    "duration": 955912,
    "start_time": "2023-08-07T20:53:11.619Z"
   },
   {
    "duration": 6,
    "start_time": "2023-08-07T21:09:25.797Z"
   },
   {
    "duration": 9,
    "start_time": "2023-08-07T21:09:32.509Z"
   },
   {
    "duration": 150097,
    "start_time": "2023-08-07T21:09:38.200Z"
   },
   {
    "duration": 162629,
    "start_time": "2023-08-07T21:12:17.263Z"
   },
   {
    "duration": 5,
    "start_time": "2023-08-07T21:15:17.385Z"
   },
   {
    "duration": 6,
    "start_time": "2023-08-07T21:15:24.327Z"
   },
   {
    "duration": 3537410,
    "start_time": "2023-08-07T21:15:28.539Z"
   },
   {
    "duration": 4,
    "start_time": "2023-08-07T22:17:00.823Z"
   },
   {
    "duration": 4,
    "start_time": "2023-08-07T22:17:05.384Z"
   },
   {
    "duration": 136926,
    "start_time": "2023-08-07T22:17:11.195Z"
   },
   {
    "duration": 144227,
    "start_time": "2023-08-07T22:20:08.445Z"
   },
   {
    "duration": 4,
    "start_time": "2023-08-07T22:25:58.489Z"
   },
   {
    "duration": 147712,
    "start_time": "2023-08-07T22:26:01.500Z"
   },
   {
    "duration": 154099,
    "start_time": "2023-08-07T22:29:32.828Z"
   },
   {
    "duration": 8,
    "start_time": "2023-08-07T22:32:15.045Z"
   },
   {
    "duration": 4,
    "start_time": "2023-08-07T22:34:27.199Z"
   },
   {
    "duration": 14759,
    "start_time": "2023-08-07T22:34:34.707Z"
   },
   {
    "duration": 105231,
    "start_time": "2023-08-07T22:35:01.451Z"
   },
   {
    "duration": 110392,
    "start_time": "2023-08-07T22:39:11.302Z"
   },
   {
    "duration": 736,
    "start_time": "2023-08-07T22:43:22.304Z"
   },
   {
    "duration": 10,
    "start_time": "2023-08-07T22:45:53.176Z"
   },
   {
    "duration": 5,
    "start_time": "2023-08-07T22:46:10.298Z"
   },
   {
    "duration": 9,
    "start_time": "2023-08-07T22:46:20.221Z"
   },
   {
    "duration": 6143,
    "start_time": "2023-08-07T22:46:31.260Z"
   },
   {
    "duration": 8124,
    "start_time": "2023-08-07T22:47:27.257Z"
   },
   {
    "duration": 240067,
    "start_time": "2023-08-07T22:48:03.350Z"
   },
   {
    "duration": 5,
    "start_time": "2023-08-07T22:52:22.009Z"
   },
   {
    "duration": 9,
    "start_time": "2023-08-07T22:52:27.116Z"
   },
   {
    "duration": 4,
    "start_time": "2023-08-07T22:52:35.366Z"
   },
   {
    "duration": 117,
    "start_time": "2023-08-07T22:52:41.731Z"
   },
   {
    "duration": 22,
    "start_time": "2023-08-07T22:53:50.083Z"
   },
   {
    "duration": 8,
    "start_time": "2023-08-07T22:58:01.529Z"
   },
   {
    "duration": 4,
    "start_time": "2023-08-07T22:58:12.345Z"
   },
   {
    "duration": 11840,
    "start_time": "2023-08-07T22:58:20.030Z"
   },
   {
    "duration": 5,
    "start_time": "2023-08-07T23:00:05.701Z"
   },
   {
    "duration": 4,
    "start_time": "2023-08-07T23:00:10.330Z"
   },
   {
    "duration": 39,
    "start_time": "2023-08-07T23:00:17.611Z"
   },
   {
    "duration": 14,
    "start_time": "2023-08-07T23:00:26.530Z"
   },
   {
    "duration": 7132,
    "start_time": "2023-08-07T23:00:35.606Z"
   },
   {
    "duration": 7068,
    "start_time": "2023-08-07T23:00:55.160Z"
   },
   {
    "duration": 4,
    "start_time": "2023-08-07T23:01:19.211Z"
   },
   {
    "duration": 3,
    "start_time": "2023-08-07T23:01:22.538Z"
   },
   {
    "duration": 14,
    "start_time": "2023-08-07T23:01:26.296Z"
   },
   {
    "duration": 16,
    "start_time": "2023-08-07T23:01:31.791Z"
   },
   {
    "duration": 8263,
    "start_time": "2023-08-07T23:01:41.732Z"
   },
   {
    "duration": 3507,
    "start_time": "2023-08-07T23:02:06.028Z"
   },
   {
    "duration": 348730,
    "start_time": "2023-08-07T23:02:16.296Z"
   },
   {
    "duration": 47,
    "start_time": "2023-08-07T23:08:31.024Z"
   },
   {
    "duration": 6,
    "start_time": "2023-08-07T23:09:07.044Z"
   },
   {
    "duration": 2004,
    "start_time": "2023-08-07T23:09:31.795Z"
   },
   {
    "duration": 8703,
    "start_time": "2023-08-07T23:09:33.802Z"
   },
   {
    "duration": 3510,
    "start_time": "2023-08-07T23:09:42.508Z"
   },
   {
    "duration": 39,
    "start_time": "2023-08-07T23:09:46.020Z"
   },
   {
    "duration": 19,
    "start_time": "2023-08-07T23:09:46.061Z"
   },
   {
    "duration": 31,
    "start_time": "2023-08-07T23:09:46.081Z"
   },
   {
    "duration": 315,
    "start_time": "2023-08-07T23:09:46.114Z"
   },
   {
    "duration": 236,
    "start_time": "2023-08-07T23:09:46.431Z"
   },
   {
    "duration": 228,
    "start_time": "2023-08-07T23:09:46.669Z"
   },
   {
    "duration": 35,
    "start_time": "2023-08-07T23:09:46.900Z"
   },
   {
    "duration": 67,
    "start_time": "2023-08-07T23:09:46.937Z"
   },
   {
    "duration": 254,
    "start_time": "2023-08-07T23:09:47.006Z"
   },
   {
    "duration": 2394,
    "start_time": "2023-08-07T23:09:47.262Z"
   },
   {
    "duration": 2666,
    "start_time": "2023-08-07T23:09:49.658Z"
   },
   {
    "duration": 5,
    "start_time": "2023-08-07T23:09:52.327Z"
   },
   {
    "duration": 2221,
    "start_time": "2023-08-07T23:09:52.334Z"
   },
   {
    "duration": 4,
    "start_time": "2023-08-07T23:09:54.557Z"
   },
   {
    "duration": 13,
    "start_time": "2023-08-07T23:09:54.563Z"
   },
   {
    "duration": 469,
    "start_time": "2023-08-07T23:09:54.578Z"
   },
   {
    "duration": 1104794,
    "start_time": "2023-08-07T23:09:55.048Z"
   },
   {
    "duration": 48,
    "start_time": "2023-08-07T23:28:19.844Z"
   },
   {
    "duration": 1081706,
    "start_time": "2023-08-07T23:28:19.909Z"
   },
   {
    "duration": 57,
    "start_time": "2023-08-07T23:46:21.617Z"
   },
   {
    "duration": 4,
    "start_time": "2023-08-07T23:46:21.675Z"
   },
   {
    "duration": 103,
    "start_time": "2023-08-07T23:46:21.680Z"
   },
   {
    "duration": 4,
    "start_time": "2023-08-07T23:46:21.784Z"
   },
   {
    "duration": 20,
    "start_time": "2023-08-07T23:46:21.789Z"
   },
   {
    "duration": 4,
    "start_time": "2023-08-07T23:46:21.812Z"
   },
   {
    "duration": 61777,
    "start_time": "2023-08-07T23:46:21.817Z"
   },
   {
    "duration": 65528,
    "start_time": "2023-08-07T23:47:23.595Z"
   },
   {
    "duration": 4,
    "start_time": "2023-08-07T23:48:29.125Z"
   },
   {
    "duration": 32,
    "start_time": "2023-08-07T23:48:29.131Z"
   },
   {
    "duration": 872877,
    "start_time": "2023-08-07T23:48:29.165Z"
   },
   {
    "duration": 909872,
    "start_time": "2023-08-08T00:03:02.044Z"
   },
   {
    "duration": 92,
    "start_time": "2023-08-08T00:18:11.918Z"
   },
   {
    "duration": 3,
    "start_time": "2023-08-08T00:18:12.012Z"
   },
   {
    "duration": 133799,
    "start_time": "2023-08-08T00:18:12.017Z"
   },
   {
    "duration": 137491,
    "start_time": "2023-08-08T00:20:25.818Z"
   },
   {
    "duration": 4,
    "start_time": "2023-08-08T00:22:43.311Z"
   },
   {
    "duration": 16,
    "start_time": "2023-08-08T00:22:43.316Z"
   },
   {
    "duration": 95910,
    "start_time": "2023-08-08T00:22:43.334Z"
   },
   {
    "duration": 98609,
    "start_time": "2023-08-08T00:24:19.246Z"
   },
   {
    "duration": 657,
    "start_time": "2023-08-08T00:25:57.857Z"
   },
   {
    "duration": 11,
    "start_time": "2023-08-08T00:25:58.516Z"
   },
   {
    "duration": 45,
    "start_time": "2023-08-08T00:25:58.528Z"
   },
   {
    "duration": 13,
    "start_time": "2023-08-08T00:25:58.574Z"
   },
   {
    "duration": 7720,
    "start_time": "2023-08-08T00:25:58.588Z"
   },
   {
    "duration": 5700,
    "start_time": "2023-08-08T00:26:06.309Z"
   },
   {
    "duration": 467204,
    "start_time": "2023-08-08T00:26:12.011Z"
   },
   {
    "duration": 386,
    "start_time": "2023-08-08T00:33:59.217Z"
   },
   {
    "duration": 0,
    "start_time": "2023-08-08T00:33:59.604Z"
   },
   {
    "duration": 0,
    "start_time": "2023-08-08T00:33:59.605Z"
   },
   {
    "duration": 31,
    "start_time": "2023-08-08T00:34:20.526Z"
   },
   {
    "duration": 30,
    "start_time": "2023-08-08T00:34:29.433Z"
   },
   {
    "duration": 11,
    "start_time": "2023-08-08T00:34:37.553Z"
   },
   {
    "duration": 3,
    "start_time": "2023-08-08T00:34:42.510Z"
   },
   {
    "duration": 57829,
    "start_time": "2023-08-08T00:34:47.873Z"
   },
   {
    "duration": 3394,
    "start_time": "2023-08-08T14:56:54.214Z"
   },
   {
    "duration": 19915,
    "start_time": "2023-08-08T14:56:57.610Z"
   },
   {
    "duration": 2942,
    "start_time": "2023-08-08T14:57:17.527Z"
   },
   {
    "duration": 39,
    "start_time": "2023-08-08T14:57:20.472Z"
   },
   {
    "duration": 15,
    "start_time": "2023-08-08T14:57:20.513Z"
   },
   {
    "duration": 14,
    "start_time": "2023-08-08T14:57:20.529Z"
   },
   {
    "duration": 325,
    "start_time": "2023-08-08T14:57:20.566Z"
   },
   {
    "duration": 239,
    "start_time": "2023-08-08T14:57:20.893Z"
   },
   {
    "duration": 262,
    "start_time": "2023-08-08T14:57:21.134Z"
   },
   {
    "duration": 29,
    "start_time": "2023-08-08T14:57:21.398Z"
   },
   {
    "duration": 34,
    "start_time": "2023-08-08T14:57:21.429Z"
   },
   {
    "duration": 216,
    "start_time": "2023-08-08T14:57:21.469Z"
   },
   {
    "duration": 1000,
    "start_time": "2023-08-08T14:57:21.688Z"
   },
   {
    "duration": 4346,
    "start_time": "2023-08-08T14:57:22.690Z"
   },
   {
    "duration": 5,
    "start_time": "2023-08-08T14:57:27.038Z"
   },
   {
    "duration": 2834,
    "start_time": "2023-08-08T14:57:27.045Z"
   },
   {
    "duration": 5,
    "start_time": "2023-08-08T14:57:29.886Z"
   },
   {
    "duration": 9,
    "start_time": "2023-08-08T14:57:29.893Z"
   },
   {
    "duration": 1126,
    "start_time": "2023-08-08T14:57:29.905Z"
   },
   {
    "duration": 1337748,
    "start_time": "2023-08-08T14:57:31.033Z"
   },
   {
    "duration": 42,
    "start_time": "2023-08-08T15:19:48.783Z"
   },
   {
    "duration": 1215460,
    "start_time": "2023-08-08T15:19:48.827Z"
   },
   {
    "duration": 53,
    "start_time": "2023-08-08T15:40:04.289Z"
   },
   {
    "duration": 4,
    "start_time": "2023-08-08T15:40:04.357Z"
   },
   {
    "duration": 108,
    "start_time": "2023-08-08T15:40:04.362Z"
   },
   {
    "duration": 4,
    "start_time": "2023-08-08T15:40:04.472Z"
   },
   {
    "duration": 16,
    "start_time": "2023-08-08T15:40:04.478Z"
   },
   {
    "duration": 8,
    "start_time": "2023-08-08T15:40:04.498Z"
   },
   {
    "duration": 70902,
    "start_time": "2023-08-08T15:40:04.508Z"
   },
   {
    "duration": 75549,
    "start_time": "2023-08-08T15:41:15.412Z"
   },
   {
    "duration": 4,
    "start_time": "2023-08-08T15:42:30.963Z"
   },
   {
    "duration": 35,
    "start_time": "2023-08-08T15:42:30.969Z"
   },
   {
    "duration": 950177,
    "start_time": "2023-08-08T15:42:31.006Z"
   },
   {
    "duration": 1060181,
    "start_time": "2023-08-08T15:58:21.186Z"
   },
   {
    "duration": 87,
    "start_time": "2023-08-08T16:16:01.376Z"
   },
   {
    "duration": 27,
    "start_time": "2023-08-08T16:16:01.465Z"
   },
   {
    "duration": 521384,
    "start_time": "2023-08-08T16:16:01.493Z"
   },
   {
    "duration": 984411,
    "start_time": "2023-08-08T16:24:42.959Z"
   },
   {
    "duration": 13,
    "start_time": "2023-08-08T16:41:07.376Z"
   },
   {
    "duration": 24,
    "start_time": "2023-08-08T16:41:07.458Z"
   },
   {
    "duration": 748017,
    "start_time": "2023-08-08T16:41:07.556Z"
   },
   {
    "duration": 815920,
    "start_time": "2023-08-08T16:53:35.575Z"
   },
   {
    "duration": 6114,
    "start_time": "2023-08-08T17:07:11.559Z"
   },
   {
    "duration": 115,
    "start_time": "2023-08-08T17:07:17.683Z"
   },
   {
    "duration": 12,
    "start_time": "2023-08-08T17:07:17.859Z"
   },
   {
    "duration": 85,
    "start_time": "2023-08-08T17:07:17.877Z"
   },
   {
    "duration": 53688,
    "start_time": "2023-08-08T17:07:17.975Z"
   },
   {
    "duration": 38511,
    "start_time": "2023-08-08T17:08:11.669Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
